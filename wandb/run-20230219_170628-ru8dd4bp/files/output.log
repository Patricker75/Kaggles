
/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/__init__.py:68: DeprecatedFeatureWarning: apex.amp is deprecated and will be removed by the end of February 2023. Use [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.
Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError("No module named 'amp_C'")
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4.0
Traceback (most recent call last):
  File "main.py", line 44, in <module>
    history = fit(epochs, net, train_ds, val_ds, device, opt = optimizer)
  File "/home/jeffreymo572/Kaggles/common/utils.py", line 128, in fit
    scaled_loss.backward()
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/contextlib.py", line 120, in __exit__
    next(self.gen)
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/amp/handle.py", line 123, in scale_loss
    optimizer._post_amp_backward(loss_scaler)
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/amp/_process_optimizer.py", line 249, in post_backward_no_master_weights
    post_backward_models_are_masters(scaler, params, stashed_grads)
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/amp/_process_optimizer.py", line 131, in post_backward_models_are_masters
    scaler.unscale_with_stashed(
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/amp/scaler.py", line 180, in unscale_with_stashed
    self.unscale_with_stashed_python(model_grads,
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/amp/scaler.py", line 143, in unscale_with_stashed_python
    self._has_overflow = axpby_check_overflow_python(model,
  File "/home/jeffreymo572/anaconda3/envs/Kaggles/lib/python3.8/site-packages/apex-0.1-py3.8.egg/apex/amp/scaler.py", line 22, in axpby_check_overflow_python
    cpu_sum = float(model_grad.float().sum())
KeyboardInterrupt